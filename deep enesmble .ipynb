{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"bugzilla.csv\")\n",
    "data_accross_project=pd.read_csv(\"columba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_accross_project.head(5)\n",
    "data_ap=data_accross_project\n",
    "data_ap=data_ap.drop('transactionid', axis=1)\n",
    "data_ap=data_ap.drop('commitdate', axis=1)\n",
    "data_ap.head()\n",
    "X_data_ap=data_ap.iloc[:,:-1]\n",
    "Y_data_ap=data_ap.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2001/12/12 17:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.579380</td>\n",
       "      <td>0.093620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>480.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>596</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>143</td>\n",
       "      <td>133.50</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1999/10/12 12:57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140</td>\n",
       "      <td>140.00</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2002/5/15 16:55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>0.739279</td>\n",
       "      <td>0.183477</td>\n",
       "      <td>0.208913</td>\n",
       "      <td>283.519231</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>15836</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>984</td>\n",
       "      <td>818.65</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2002/1/21 15:37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.685328</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>514.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>579</td>\n",
       "      <td>479.25</td>\n",
       "      <td>550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2001/12/19 16:44</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>0.769776</td>\n",
       "      <td>0.091829</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>366.815789</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>6565</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>413</td>\n",
       "      <td>313.25</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid        commitdate  ns  nm  nf   entropy        la        ld  \\\n",
       "0              3  2001/12/12 17:41   1   1   3  0.579380  0.093620  0.000000   \n",
       "1              7  1999/10/12 12:57   1   1   1  0.000000  0.000000  0.000000   \n",
       "2              8   2002/5/15 16:55   3   3  52  0.739279  0.183477  0.208913   \n",
       "3              9   2002/1/21 15:37   1   1   8  0.685328  0.016039  0.012880   \n",
       "4             10  2001/12/19 16:44   2   2  38  0.769776  0.091829  0.072746   \n",
       "\n",
       "           lt  fix  ndev     pd       npt  exp    rexp  sexp  bug  \n",
       "0  480.666667    1    14    596  0.666667  143  133.50   129    1  \n",
       "1  398.000000    1     1      0  1.000000  140  140.00   137    1  \n",
       "2  283.519231    0    23  15836  0.750000  984  818.65   978    0  \n",
       "3  514.375000    1    21   1281  1.000000  579  479.25   550    0  \n",
       "4  366.815789    1    21   6565  0.763158  413  313.25   405    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "\n",
    "# total bug data classes 1696 rows.\n",
    "non_bug_data = data.loc[data['bug'] == 0][:1000]\n",
    "bug = data.loc[data['bug'] == 1][:1000]\n",
    "test_data_1=data.loc[data['bug'] == 1][1000:1696]\n",
    "test_data_0 = data.loc[data['bug'] == 0][1000:-1]\n",
    "pure_data = pd.concat([non_bug_data,bug])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "pure_data = pure_data.sample(frac=1, random_state=7)\n",
    "pure_data=pure_data.drop(\"transactionid\", axis=1)\n",
    "pure_data=pure_data.drop(\"commitdate\", axis=1)\n",
    "\n",
    "X=pure_data.iloc[:,:-1]\n",
    "Y=pure_data.iloc[:,-1]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_enesmble(X_train,Y_train):\n",
    "    l=[]\n",
    "    X_train1,X_test1,Y_train1,Y_test1=train_test_split(X_train,Y_train,test_size=0.2,random_state=20)\n",
    "    model=Sequential()\n",
    "    model.add(Dense(128, init=\"uniform\", input_dim=14, activation='relu'))\n",
    "    model.add(Dense(64, init =\"uniform\", activation=\"relu\"))\n",
    "    model.add(Dense(1, init=\"uniform\", activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['accuracy'], optimizer='adam')\n",
    "    history=model.fit(X_train1,Y_train1, epochs=100, batch_size=100)\n",
    "    Y_pred1=model.predict(X_test1)\n",
    "    Y_pred1=(Y_pred1>0.5)\n",
    "    model3=RandomForestClassifier()\n",
    "    model3.fit(X_train,Y_train)\n",
    "    Y_pred3=model3.predict(X_test1)\n",
    "    model2 = XGBClassifier()\n",
    "    model2.fit(X_train1, Y_train1)\n",
    "    Y_pred_xg=model2.predict(X_test1)\n",
    "    accuracy_1=accuracy_score(Y_test1,Y_pred_xg)\n",
    "    accuracy_2=accuracy_score(Y_test1,Y_pred1)\n",
    "    accuracy_3=accuracy_score(Y_test1,Y_pred3)\n",
    "    p1=precision_score(Y_test1,Y_pred1)\n",
    "    p2=precision_score(Y_test1,Y_pred_xg)\n",
    "    p3=precision_score(Y_test1,Y_pred3)\n",
    "    \n",
    "    f1=f1_score(Y_test1,Y_pred1)\n",
    "    f2=f1_score(Y_test1,Y_pred_xg)\n",
    "    f3=f1_score(Y_test1,Y_pred3)\n",
    "    \n",
    "    cm=confusion_matrix(Y_test1.tolist(),Y_pred1.tolist())\n",
    "    cm2=confusion_matrix(Y_test1.tolist(),Y_pred_xg.tolist())\n",
    "    cm3=confusion_matrix(Y_test1.tolist(),Y_pred3.tolist())\n",
    "    \n",
    "    total=sum(sum(cm))\n",
    "    sensitivity1 = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "    total2=sum(sum(cm2))\n",
    "    sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[1,0])\n",
    "    sensitivity3 = cm3[0,0]/(cm3[0,0]+cm3[1,0])\n",
    "    \n",
    "    specificity1 = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "    specificity2 = cm2[1,1]/(cm2[1,1]+cm2[0,1])\n",
    "    specificity3 = cm3[1,1]/(cm3[1,1]+cm3[0,1])\n",
    "    \n",
    "    total=sum(sum(cm3))\n",
    "\n",
    "    return ((accuracy_1+accuracy_2+accuracy_3)/3),(p1+p2+p3)/3, (f1+f2+f3)/3, (specificity1+sensitivity2+sensitivity3)/3, (specificity1+specificity2+specificity3)/3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=14, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 0s 191us/step - loss: 0.7049 - acc: 0.5430\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.6987 - acc: 0.5742\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.6721 - acc: 0.5828\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.6485 - acc: 0.6039\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6420 - acc: 0.6031\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6455 - acc: 0.6227\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.6489 - acc: 0.6047\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6469 - acc: 0.6195\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.6413 - acc: 0.6258\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.6441 - acc: 0.6102\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.6491 - acc: 0.6188\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.6265 - acc: 0.6266\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6343 - acc: 0.6203\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.6103 - acc: 0.6461\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.6100 - acc: 0.6461\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.6095 - acc: 0.6414\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.6054 - acc: 0.6516\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.6041 - acc: 0.6437\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.6159 - acc: 0.6289\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6311 - acc: 0.6406\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.6108 - acc: 0.6516\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.6120 - acc: 0.6437\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6116 - acc: 0.6344\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.6009 - acc: 0.6469\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.6034 - acc: 0.6406\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5869 - acc: 0.6578\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5879 - acc: 0.6633\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5924 - acc: 0.6562\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 0s 26us/step - loss: 0.5928 - acc: 0.6555\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5913 - acc: 0.6516\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5998 - acc: 0.6320\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 0s 26us/step - loss: 0.6019 - acc: 0.6477\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5823 - acc: 0.6742\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5867 - acc: 0.6648\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 0s 26us/step - loss: 0.5863 - acc: 0.6703\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.5925 - acc: 0.6516\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5870 - acc: 0.6492\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5742 - acc: 0.6758\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5735 - acc: 0.6664\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5693 - acc: 0.6719\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5719 - acc: 0.6742\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5882 - acc: 0.6578\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5805 - acc: 0.6617\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5827 - acc: 0.6617\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5620 - acc: 0.6883\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5608 - acc: 0.6766\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5727 - acc: 0.6789\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5739 - acc: 0.6750\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.5641 - acc: 0.6812\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5623 - acc: 0.6664\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5610 - acc: 0.6719\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.5724 - acc: 0.6766\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.5660 - acc: 0.6820\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5513 - acc: 0.6828\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 0s 16us/step - loss: 0.5492 - acc: 0.6938\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5509 - acc: 0.6805\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5518 - acc: 0.6914\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5514 - acc: 0.6867\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5640 - acc: 0.6758\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5431 - acc: 0.6891\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5442 - acc: 0.6945\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5424 - acc: 0.6953\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5461 - acc: 0.6969\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 0s 19us/step - loss: 0.5367 - acc: 0.6945\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5531 - acc: 0.6805\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5519 - acc: 0.6797\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 0s 32us/step - loss: 0.5479 - acc: 0.6938\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5387 - acc: 0.6930\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5323 - acc: 0.6961\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5459 - acc: 0.6891\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5278 - acc: 0.7055\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 0s 26us/step - loss: 0.5414 - acc: 0.6852\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.5406 - acc: 0.6844\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 0s 25us/step - loss: 0.5437 - acc: 0.6867\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 0s 24us/step - loss: 0.5353 - acc: 0.6945\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5471 - acc: 0.6953\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5343 - acc: 0.6922\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 0s 24us/step - loss: 0.5375 - acc: 0.6922\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.5446 - acc: 0.6922\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5404 - acc: 0.7039\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5292 - acc: 0.6969\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 0s 24us/step - loss: 0.5304 - acc: 0.7156\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5296 - acc: 0.7109\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5199 - acc: 0.7141\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5150 - acc: 0.7141\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 0s 24us/step - loss: 0.5187 - acc: 0.7070\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 0s 23us/step - loss: 0.5129 - acc: 0.7234\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.5169 - acc: 0.7187\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5191 - acc: 0.7086\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 0s 22us/step - loss: 0.5071 - acc: 0.7180\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 0s 20us/step - loss: 0.5129 - acc: 0.7172\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 0s 21us/step - loss: 0.5224 - acc: 0.7227\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5155 - acc: 0.7180\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5087 - acc: 0.7070\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5012 - acc: 0.7391\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5088 - acc: 0.7219\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 0s 15us/step - loss: 0.5482 - acc: 0.6898\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5349 - acc: 0.7109\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 0s 18us/step - loss: 0.5294 - acc: 0.7055\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 0s 17us/step - loss: 0.5061 - acc: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d,e=deep_enesmble(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7739583333333333,\n",
       " 0.7842245035826219,\n",
       " 0.7546382257907908,\n",
       " 0.7798321506108032,\n",
       " 0.7842245035826219)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(pure_data, n):\n",
    "    j=0\n",
    "    accuracy=0\n",
    "    p=0\n",
    "    f1_score=0\n",
    "    sensitivity=0\n",
    "    specificity=0\n",
    "    for i in range(n):\n",
    "        x1=pure_data.iloc[j:j+50,:-1]\n",
    "        y1=pure_data.iloc[j:j+50,-1]\n",
    "        j=j+50\n",
    "        a,p1,f,sens,spec=deep_enesmble(x1,y1)\n",
    "        accuracy=a+accuracy\n",
    "        p=p+p1\n",
    "        f1_score=f1_score+f\n",
    "        sensitivity=sens+sensitivity\n",
    "        specificity=specificity+spec\n",
    "    print(\"Accuracy\",accuracy)\n",
    "    print(\"P score\", p)\n",
    "    print(\"f1_score\",f1_score)\n",
    "    print(\"Sensivity\",sensitivity)\n",
    "    print(\"specificity\",specificity)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHECKING FOR THE PERFORMANCE USING DIFFERENT WHOLE DATASET\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=14, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3564/3564 [==============================] - 0s 83us/step - loss: 1.3794 - acc: 0.6164\n",
      "Epoch 2/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.7673 - acc: 0.6411\n",
      "Epoch 3/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6622 - acc: 0.6911\n",
      "Epoch 4/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.7199 - acc: 0.6692\n",
      "Epoch 5/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.7215 - acc: 0.6580\n",
      "Epoch 6/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6538 - acc: 0.6930\n",
      "Epoch 7/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6681 - acc: 0.6860\n",
      "Epoch 8/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6582 - acc: 0.6846\n",
      "Epoch 9/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6263 - acc: 0.7079\n",
      "Epoch 10/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6256 - acc: 0.7059\n",
      "Epoch 11/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6427 - acc: 0.6905\n",
      "Epoch 12/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.6359 - acc: 0.6964\n",
      "Epoch 13/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6356 - acc: 0.7015\n",
      "Epoch 14/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.6165 - acc: 0.7023\n",
      "Epoch 15/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6105 - acc: 0.7118\n",
      "Epoch 16/100\n",
      "3564/3564 [==============================] - 0s 16us/step - loss: 0.6172 - acc: 0.7079\n",
      "Epoch 17/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.6164 - acc: 0.7068\n",
      "Epoch 18/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6068 - acc: 0.7031\n",
      "Epoch 19/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6015 - acc: 0.7132\n",
      "Epoch 20/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6126 - acc: 0.7074\n",
      "Epoch 21/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6062 - acc: 0.7079\n",
      "Epoch 22/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6045 - acc: 0.7152\n",
      "Epoch 23/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.6029 - acc: 0.7104\n",
      "Epoch 24/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5968 - acc: 0.7149\n",
      "Epoch 25/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.6130 - acc: 0.7034\n",
      "Epoch 26/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5952 - acc: 0.7099\n",
      "Epoch 27/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.6021 - acc: 0.7088\n",
      "Epoch 28/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5993 - acc: 0.7102\n",
      "Epoch 29/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5868 - acc: 0.7163\n",
      "Epoch 30/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5918 - acc: 0.7121\n",
      "Epoch 31/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5850 - acc: 0.7155\n",
      "Epoch 32/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.5852 - acc: 0.7219\n",
      "Epoch 33/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5958 - acc: 0.7138\n",
      "Epoch 34/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5909 - acc: 0.7132\n",
      "Epoch 35/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5806 - acc: 0.7189\n",
      "Epoch 36/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5898 - acc: 0.7177\n",
      "Epoch 37/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5959 - acc: 0.7169\n",
      "Epoch 38/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.6072 - acc: 0.7071\n",
      "Epoch 39/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5793 - acc: 0.7217\n",
      "Epoch 40/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5924 - acc: 0.7183\n",
      "Epoch 41/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.5807 - acc: 0.7203\n",
      "Epoch 42/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5766 - acc: 0.7245\n",
      "Epoch 43/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5865 - acc: 0.7214\n",
      "Epoch 44/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5828 - acc: 0.7152\n",
      "Epoch 45/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5750 - acc: 0.7259\n",
      "Epoch 46/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5749 - acc: 0.7247\n",
      "Epoch 47/100\n",
      "3564/3564 [==============================] - 0s 17us/step - loss: 0.5745 - acc: 0.7231\n",
      "Epoch 48/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5778 - acc: 0.7155\n",
      "Epoch 49/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5844 - acc: 0.7186\n",
      "Epoch 50/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.5770 - acc: 0.7205\n",
      "Epoch 51/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5622 - acc: 0.7270\n",
      "Epoch 52/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5646 - acc: 0.7298\n",
      "Epoch 53/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5618 - acc: 0.7309\n",
      "Epoch 54/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5592 - acc: 0.7270\n",
      "Epoch 55/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5649 - acc: 0.7318\n",
      "Epoch 56/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5635 - acc: 0.7301\n",
      "Epoch 57/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5702 - acc: 0.7236\n",
      "Epoch 58/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5605 - acc: 0.7323\n",
      "Epoch 59/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5648 - acc: 0.7281\n",
      "Epoch 60/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5638 - acc: 0.7295\n",
      "Epoch 61/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5552 - acc: 0.7332\n",
      "Epoch 62/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5602 - acc: 0.7273\n",
      "Epoch 63/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5676 - acc: 0.7236\n",
      "Epoch 64/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5590 - acc: 0.7309\n",
      "Epoch 65/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5559 - acc: 0.7363\n",
      "Epoch 66/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5577 - acc: 0.7306\n",
      "Epoch 67/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5604 - acc: 0.7306\n",
      "Epoch 68/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5570 - acc: 0.7357\n",
      "Epoch 69/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5575 - acc: 0.7346\n",
      "Epoch 70/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5621 - acc: 0.7256\n",
      "Epoch 71/100\n",
      "3564/3564 [==============================] - 0s 15us/step - loss: 0.5610 - acc: 0.7304\n",
      "Epoch 72/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5556 - acc: 0.7365\n",
      "Epoch 73/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5542 - acc: 0.7267\n",
      "Epoch 74/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5560 - acc: 0.7354\n",
      "Epoch 75/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5566 - acc: 0.7262\n",
      "Epoch 76/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5505 - acc: 0.7337\n",
      "Epoch 77/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5588 - acc: 0.7318\n",
      "Epoch 78/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5547 - acc: 0.7309\n",
      "Epoch 79/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5506 - acc: 0.7340\n",
      "Epoch 80/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5558 - acc: 0.7253\n",
      "Epoch 81/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5567 - acc: 0.7323\n",
      "Epoch 82/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5448 - acc: 0.7365\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5558 - acc: 0.7346\n",
      "Epoch 84/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5480 - acc: 0.7332\n",
      "Epoch 85/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5565 - acc: 0.7301\n",
      "Epoch 86/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5437 - acc: 0.7379\n",
      "Epoch 87/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5428 - acc: 0.7416\n",
      "Epoch 88/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5406 - acc: 0.7385\n",
      "Epoch 89/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5779 - acc: 0.7256\n",
      "Epoch 90/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5534 - acc: 0.7368\n",
      "Epoch 91/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5458 - acc: 0.7374\n",
      "Epoch 92/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5485 - acc: 0.7407\n",
      "Epoch 93/100\n",
      "3564/3564 [==============================] - 0s 14us/step - loss: 0.5486 - acc: 0.7374\n",
      "Epoch 94/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5601 - acc: 0.7360\n",
      "Epoch 95/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5415 - acc: 0.7402\n",
      "Epoch 96/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5493 - acc: 0.7357\n",
      "Epoch 97/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5414 - acc: 0.7430\n",
      "Epoch 98/100\n",
      "3564/3564 [==============================] - 0s 13us/step - loss: 0.5386 - acc: 0.7435\n",
      "Epoch 99/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5360 - acc: 0.7489\n",
      "Epoch 100/100\n",
      "3564/3564 [==============================] - 0s 12us/step - loss: 0.5372 - acc: 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "a2,b2,c2,d2,e2=deep_enesmble(X_data_ap,Y_data_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8185559296670407,\n",
       " 0.7993556892453951,\n",
       " 0.5860345999347204,\n",
       " 0.8162019780533932,\n",
       " 0.7993556892453951)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2,b2,c2,d2,e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
